{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d47e5f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emman\\Desktop\\PROYECTOS_VS_CODE\\PRUEBAS_DE_PYTHON\\PORTFOLIO-venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\emman\\Desktop\\PROYECTOS_VS_CODE\\PRUEBAS_DE_PYTHON\\PORTFOLIO-venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\emman\\.cache\\huggingface\\hub\\datasets--Abirate--english_quotes. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 2508/2508 [00:00<00:00, 338350.42 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sequences: 77,981 | Max seq len: 750 | Vocab size: 8,109\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emman\\Desktop\\PROYECTOS_VS_CODE\\PRUEBAS_DE_PYTHON\\PORTFOLIO-venv\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 1s/step - loss: 6.6885 - sparse_categorical_accuracy: 0.0393 - val_loss: 6.5493 - val_sparse_categorical_accuracy: 0.0403 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 1s/step - loss: 6.2049 - sparse_categorical_accuracy: 0.0618 - val_loss: 6.4125 - val_sparse_categorical_accuracy: 0.0736 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 1s/step - loss: 5.9568 - sparse_categorical_accuracy: 0.0834 - val_loss: 6.2936 - val_sparse_categorical_accuracy: 0.0913 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 1s/step - loss: 5.7315 - sparse_categorical_accuracy: 0.1028 - val_loss: 6.2472 - val_sparse_categorical_accuracy: 0.1032 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 1s/step - loss: 5.5412 - sparse_categorical_accuracy: 0.1173 - val_loss: 6.2215 - val_sparse_categorical_accuracy: 0.1069 - learning_rate: 0.0010\n",
      "\n",
      "--- Samples ---\n",
      "Once upon a time to loving its only song may not find the friends like contained ” because i can loved you ” to\n",
      "The meaning of life is your hell have to how high it i'm darkness from his matter a wave life fear than one hero ”\n",
      "Happiness comes from our own part of the horror of her ” my mind was the human a triumph is you never remember\n"
     ]
    }
   ],
   "source": [
    "# If needed (first run):\n",
    "# !pip install -q datasets\n",
    "\n",
    "# ===== 1) Load and prepare text =====\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Load a small quotes dataset\n",
    "ds = load_dataset(\"Abirate/english_quotes\")\n",
    "quotes = ds[\"train\"][\"quote\"]\n",
    "\n",
    "# (Optional) you can filter very short quotes to reduce noise\n",
    "quotes = [q.strip() for q in quotes if isinstance(q, str) and len(q.split()) >= 3]\n",
    "\n",
    "# ===== 2) Tokenize and build n-gram training sequences =====\n",
    "# Cap the vocab to keep the model small; increase if you like\n",
    "VOCAB_CAP = 20000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_CAP, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(quotes)\n",
    "\n",
    "sequences = []\n",
    "for q in quotes:\n",
    "    token_list = tokenizer.texts_to_sequences([q])[0]\n",
    "    # build n-grams: [w1,w2] -> label w3 ; [w1,w2,w3] -> label w4 ; ...\n",
    "    for i in range(1, len(token_list)):\n",
    "        ngram = token_list[: i + 1]  # includes the label at the end\n",
    "        sequences.append(ngram)\n",
    "\n",
    "# Pad to the longest sequence length\n",
    "max_seq_len = max(len(s) for s in sequences)\n",
    "sequences = pad_sequences(sequences, maxlen=max_seq_len, padding=\"pre\")\n",
    "\n",
    "# Inputs are all tokens except last; labels are last token\n",
    "X = sequences[:, :-1]\n",
    "y = sequences[:, -1]  # integers\n",
    "vocab_size = min(VOCAB_CAP, len(tokenizer.word_index) + 1)\n",
    "\n",
    "print(f\"Num sequences: {len(sequences):,} | Max seq len: {max_seq_len} | Vocab size: {vocab_size:,}\")\n",
    "\n",
    "# ===== 3) Build & train the LSTM next-word model =====\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_seq_len - 1),\n",
    "    LSTM(256),\n",
    "    Dense(vocab_size, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",  # y is integer class id\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    validation_split=0.1,\n",
    "    epochs=5,            # bump up to 20+ for better results\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===== 4) Text generation with temperature sampling =====\n",
    "index_to_word = tokenizer.index_word  # maps token id -> word\n",
    "\n",
    "def sample_from_probs(probs, temperature: float = 1.0, rng: np.random.Generator | None = None):\n",
    "    \"\"\"Sample an index from a probability vector with temperature.\"\"\"\n",
    "    if temperature <= 0:\n",
    "        return int(np.argmax(probs))\n",
    "    logits = np.log(np.maximum(probs, 1e-9)) / float(temperature)\n",
    "    exp = np.exp(logits - np.max(logits))\n",
    "    p = exp / np.sum(exp)\n",
    "    rng = rng or np.random.default_rng()\n",
    "    return int(rng.choice(len(p), p=p))\n",
    "\n",
    "def generate_text(seed_text: str, num_words: int, temperature: float = 0.8):\n",
    "    \"\"\"\n",
    "    Greedy/temperature sampling next-word generation.\n",
    "    - seed_text: starting prompt\n",
    "    - num_words: how many words to add\n",
    "    - temperature: lower = greedier, higher = more random (e.g. 0.7–1.2)\n",
    "    \"\"\"\n",
    "    text = seed_text.strip()\n",
    "    for _ in range(num_words):\n",
    "        # encode and pad\n",
    "        seq = tokenizer.texts_to_sequences([text])[0]\n",
    "        if not seq:\n",
    "            # if all words OOV, start fresh with OOV token\n",
    "            seq = [tokenizer.word_index.get(\"<OOV>\", 1)]\n",
    "        seq = pad_sequences([seq], maxlen=max_seq_len - 1, padding=\"pre\")\n",
    "\n",
    "        # predict next-token distribution\n",
    "        preds = model.predict(seq, verbose=0)[0]  # (vocab_size,)\n",
    "        next_id = sample_from_probs(preds, temperature=temperature)\n",
    "\n",
    "        # map id->word; if missing, skip\n",
    "        word = index_to_word.get(next_id, \"\")\n",
    "        if not word:\n",
    "            # fallback to argmax if sampled OOV or unknown index\n",
    "            next_id = int(np.argmax(preds))\n",
    "            word = index_to_word.get(next_id, \"\")\n",
    "            if not word:\n",
    "                break\n",
    "        text += \" \" + word\n",
    "    return text\n",
    "\n",
    "# ===== 5) Demo generation =====\n",
    "print(\"\\n--- Samples ---\")\n",
    "print(generate_text(\"Once upon a time\", 20, temperature=0.8))\n",
    "print(generate_text(\"The meaning of life is\", 20, temperature=0.9))\n",
    "print(generate_text(\"Happiness comes from\", 20, temperature=0.7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de62795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def generate_text(seed_text: str, num_words: int, temperature: float = 0.8, wrap_width: int = 80):\n",
    "    \"\"\"\n",
    "    Greedy/temperature sampling next-word generation.\n",
    "    - seed_text: starting prompt\n",
    "    - num_words: how many words to add\n",
    "    - temperature: lower = greedier, higher = more random (e.g. 0.7–1.2)\n",
    "    - wrap_width: max characters per line for output formatting\n",
    "    \"\"\"\n",
    "    text = seed_text.strip()\n",
    "    for _ in range(num_words):\n",
    "        # encode and pad\n",
    "        seq = tokenizer.texts_to_sequences([text])[0]\n",
    "        if not seq:\n",
    "            seq = [tokenizer.word_index.get(\"<OOV>\", 1)]\n",
    "        seq = pad_sequences([seq], maxlen=max_seq_len - 1, padding=\"pre\")\n",
    "\n",
    "        # predict next-token distribution\n",
    "        preds = model.predict(seq, verbose=0)[0]\n",
    "        next_id = sample_from_probs(preds, temperature=temperature)\n",
    "\n",
    "        word = index_to_word.get(next_id, \"\")\n",
    "        if not word:\n",
    "            next_id = int(np.argmax(preds))\n",
    "            word = index_to_word.get(next_id, \"\")\n",
    "            if not word:\n",
    "                break\n",
    "        text += \" \" + word\n",
    "\n",
    "    # Format output\n",
    "    print(\"Starting text:\")\n",
    "    print(textwrap.fill(seed_text.strip(), width=wrap_width))\n",
    "    print(\"\\nGenerated text:\")\n",
    "    generated_only = text[len(seed_text.strip()):].strip()\n",
    "    print(textwrap.fill(generated_only, width=wrap_width))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "805bfcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text:\n",
      "Once upon a time\n",
      "\n",
      "Generated text:\n",
      "of so all that does to something jace like you can find are the own minded what\n",
      "me her sit nor have so right so not as everything we have be mind ” sam ” when\n",
      "that 'i running down to apollo and much myth was happy ” lies teaches\n",
      "\n",
      "\n",
      "Starting text:\n",
      "The meaning of life is\n",
      "\n",
      "Generated text:\n",
      "a things will be ” the pursuing if you mean it ” their way you can take my\n",
      "single day it down in breathlessness how ” joy is the best tasted not can even ”\n",
      "into a lease or your one natural when taken to wrap the defeats ” the\n",
      "\n",
      "\n",
      "Starting text:\n",
      "Happiness comes from\n",
      "\n",
      "Generated text:\n",
      "men ” love that i make someone reading i feel our dna ” with losing the sure ”\n",
      "that nothing is to rather and someone to me if i speak that i say anything to be\n",
      "half ” ” ” ” in the own way ” i can be no\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_text(\"Once upon a time\", 50, temperature=0.9)\n",
    "generate_text(\"The meaning of life is\", 50, temperature=0.8)\n",
    "generate_text(\"Happiness comes from\", 50, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a59d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PORTFOLIO-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
